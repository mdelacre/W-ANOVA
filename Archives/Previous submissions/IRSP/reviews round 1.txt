Dear Dr. Leys,

Thank you for submitting your manuscript to the International Review of Social Psychology. I have now received comments from two reviewers with expertise in this area and also read your paper carefully myself. The reviewers' comments are appended below. 
As you can see, the reviewers identify some strengths in your paper (e.g., practically relevant suggestions, transparent contribution). However, they both have concerns about the manuscript: Whereas Reviewer 1 thinks your paper is potentially publishable pending major revision, Reviewer 2 has some doubts about its suitability for publication. Based on my own reading, I share most of the Reviewers’ concerns but think that your paper could be an interesting contribution after an extensive revision. Accordingly, I am inviting you to revise your paper and resubmit it for further consideration.

Both reviewers made very explicit comments, so I will not reiterate them. To sum up, Reviewer 1 thinks that some conclusions are not fully supported by the data and that some (important) results are not discussed in the document. I would add that I personally found difficult to identify the relevant results in the Tables. I thus think that the presentation of results and their discussion should be deeply modified. Reviewer 2 points out that the document contains misconceptions or conceptual errors that undermine the pedagogical aim of the paper. In this regard, I would add that the International Review of Social Psychology publishes methodological articles that present in a pedagogical way statistical tools useful to social psychologists. In my opinion, the manuscript is currently quite difficult for non-specialists to read (especially the results). In other words, I think that the manuscript should be substantially modified in order to increase its pedagogical usefulness.

If you intend to resubmit your paper, please let me know of your intentions as soon as possible. If so, please could I have your revised manuscript within 90 days. If you cannot make this deadline, please let me know as early as possible.

I would like to take this opportunity to thank you for considering the International Review of Social Psychology as an outlet for your work and I look forward to receiving your revised manuscript in due course.

Yours sincerely

Benoît Dompnier
Université de Lausanne
benoit.dompnier@unil.ch
------------------------------------------------------
Reviewer 1:

	I have read this manuscript with interest. It is a technically competent contribution providing practically relevant suggestions. Despite many positive features, I think that there are few major issues that need to be addressed. The most important of them all is the key message of this contribution: after reading this paper, as well as other relevant papers on the same or related issues, I am not convinced that one should always use the W-test instead of the F-test.
The authors provide a compelling introduction and present very well their contribution. They should also be complimented in the transparency of the contribution and in making available the more extensive set of results in the SM and as additional material in the OSF.
In my opinion, the main problem is in the discussion of (some) of the results. 
After having inspected the Excel table of the Type I error in the OSF as referred to in the SM, I am not sure that the main gist (the three main points) in the manuscript is an accurate reflection of some of the results. My reading of the Excel table is that when the homoscedasticity assumption is respected (i.e., variances are equal across groups), the F-test always outperforms the W-test (and the F* test). The gap in performance increases as the number of groups grow and holds regardless of whether the sample sizes are unequal (i.e., for both balanced and unbalanced designs) and for all types of distribution tested. The gap tends to increase with higher skewness, both positive and negative, with more groups, and with smaller sample sizes. These results are not reported nor discussed in the main manuscript. 
The second point presented is a correct description of the results, that is the Type I error of the F-test is heavily affected by heteroscedasticity, with both over and underestimation depending on the direction of the correlation between N and SD. The W-test is better in this respect. 
Concerning the third point (for highly skewed distributions the W-test is better), I could not see in the Excel table results supporting it. The results reported in the manuscript (Table 4) is about one combination of highly skewed scores with unequal variances, which is not the same as the point made. I believe that these results may be driven by unequal variances (in line with the results summed up in point 2), regardless of whether the design is balanced. Perhaps I have missed something in the Excel table, or perhaps the results are not reported there, but I could not see the summary results of a simulation with high skewness, but equal variances, allowing to make this point.
The results for power are presented better in the corresponding table in the OSF than in the manuscript. As for the Type I error, the easier to understand presentation is in terms of reference with theoretical power rather than comparing the tests. This latter way of presenting the results does not allow to understand easily which test performs better. Moreover, it is inconsistent with how results are presented for Type I error. I think that the tables should be corrected and the discussion revised.
Anyway, the discussion concerning the results for power is slightly more balanced but still reflects only partly the full results in the Excel tables in OSF. The F test performs almost always better than the W test for three groups, even in the presence of heteroscedasticity. In some cases, the difference is substantial. The gap in performance tends to diminish or disappear with increasing sample sizes. These results are not mentioned in the main text. For two groups the results are more balanced. 
The final recommendations, therefore, are a mixed bag. The first recommendation does not reflect the more nuanced results. In a nutshell, from my reading of the tables in OSF, the full results seem to suggest that the F-test performs better than the W test when variances are equal, both for Type I and power (not always for this latter, but almost invariably, and sometimes substantially, with three groups). The W test instead is generally better in the presence of heteroscedasticity, but not concerning power with three groups. I could not see clear results supporting the point that the W test performs better than the F test with high skewness when considered by itself and not in combination with other aspects.
These results suggest that there is no overall “winner”, but it depends on a combination of features. This is in contrast with the current main message of the contribution.
The results also seem roughly in line with previous simulations and contributions, some of which have not been considered in this contribution (Harwell et al., 1992; Wilcox et al., 1986).
Finally, concerning the issue of the problems in adopting a two-step approach to decide which test is better depending on the homoscedasticity assumption (pp. 3-4), there is a recent contribution suggesting a procedure that seems to represent a potentially viable solution (Kim & Cribbie, 2018).
Incidentally, there is a reference to Figures 1 and 2 that is either misplaced, and there are no such figures, or the figures are missing (or anyway, I could not find them).
To summarize, I like the basic idea of this work, and I appreciate the transparency in making available all results. However, I think that the contribution should be revised, most importantly by providing a much more nuanced picture of the results and henceforth more nuanced recommendations that line up better with them.   

References
Harwell, M. R., Rubinstein, E. N., Hayes, W. S., & Olds, C. C. (1992). Summarizing Monte Carlo results in methodological research: The one-and two-factor fixed effects ANOVA cases. Journal of educational statistics, 17(4), 315-339.
Kim, Y. J., & Cribbie, R. A. (2018). ANOVA and the variance homogeneity assumption: Exploring a better gatekeeper. British Journal of Mathematical and Statistical Psychology, 71(1), 1-12.
Wilcox, R. R., Charlin, V. L., & Thompson, K. L. (1986). New monte carlo results on the robustness of the anova f, w and f statistics. Communications in Statistics-Simulation and Computation, 15(4), 933-943.

------------------------------------------------------

------------------------------------------------------
Reviewer 2:

	In the context of experimental psychology, the article underlines the need to not take linear model assumptions for granted, and not to stick to the classical F-test for one-way ANOVA (with F* and W alternative tests considered). Monte Carlo simulations are run to add (computational) empirical evidence to past research and mathematical derivations in terms of type I and II error rates.

The Welch adaptations of classical t-test and ANOVA have been available for years (even in SPSS) and walkthrough provided in many textbooks and online guides to applied statistics in psychology. The same is true for the recommendation of relying on descriptives to characterize shape and choose test accordingly, using Shapiro-Wilk for detecting departures from normality, or generally adopting a Neyman-Pearson approach to data testing (power analysis and thus control of type I/II error rates).

Yet, the paper could be yet another useful attempt at pushing psychology researchers to switch to more flexible/robust methods, if it provided:
- better articulation with the Delacre (2017) paper (since simply extending from the t-test to ANOVA)
- comparison to nonparametric tests (as done in previous articles on the topic, including those from the same authors)
- better referencing of past statistical research (of course from statistics, but also from empirical research methods themselves, even in psychology)

Also, in its current state, there are many misconceptions (although often taken from more or less recent articles) as well as conceptual errors in the article, that may not serve its potential educational purpose. There are also missing figures, and the github source code does not serve the paper and replicability to its best.

Finally, in 2018, and even though t-test and ANOVA are still recurrent in experimental psychology, research methods papers should consider more general classes of models or methods. This is not only to consider more realistic designs in social psychology, but also for avoiding many of the pitfalls mentioned in the paper (especially since many of them are already used in social psychology):
- linear model (minimally)
- mixed models (allowing to control for more factors, thus preventing excessive mixed-normal distributions)
- generalized models (for link functions and transformations avoiding both anormality and heteroscedasticity)
- weighted regression (to prevent heteroscedasticity due to outliers)
- Eicker or White-estimator (to get heteroscedasticity-consistent standard errors)

You will find additional details and more precise comments in the attached file. Due to the many issues already pointed out in the initial sections of the paper, only the main comments are reported in the detailed report on the technical parts(formulae, simulations, results).

I would therefore recommend rejecting the paper. To wrap up, for the paper to be really useful as yet another argument in favor of turning away from the old classical F-test for psychologists, it should get a bibliographical update, improved comparisons and simulation plan, as well as a user-friendly way of running simulation experiments and getting convinced by the results (not even thinking of R/Shiny interface or a code abstraction layer).
--------------------------------------------------------
Content (page: comment)
--------------------------------------------------------
a: "Parametric tests rely on two main assumptions: normality of the distribution and equality of variances." => This is plain wrong and should be reformuled. Within the very large class of parametric tests, this does not even hold for tests relying on the exponential family of probability distributions. And most importantly, it is of course not true for the Welch-adaptations, which are parametric.

3: What is the data that should be normal for ANOVA? Not the IV of course, not the DV; only the DV conditioned by the IV(s), i.e. the residuals (as in any linear model)
3: Add i.i.d. to the clasical ANOVA assumptions (since independence is missing), even if not directly studied in this paper
3: "quite robust conclusions" = in terms of type I or II errors ? This was already differentiated in Glass et al. (1972), and studied since then.
3: "lack of attention to both the homoscedasticity and the normality assumptions" = how can this be inferred without error from non corrected dfs in papers? Since these corrections mostly are continuous and non linear, applying them by default may be the best option, but statistical softwares used in psychology and associated test procedures push to differentiate the reported statistics based on heteroscedasticity test results (e.g. Levene's based on mean, BrownñForsythe based on median). This sometimes leads to detectable inconsistencies between reported F, degrees of freedom and p-values, that may simply be due to applied correction (without reporting the adequate dfs)
3: Why not considering nonparametric tests? They are also a standard alternative to violation of assumed distribution (here normality), often better deal with heteroscedasticity in terms of inference (e.g. old series of papers by Edgington and Onghena, even though distribution similarity across conditions remains an assumption), while being less powerful only in specific situations (small samples sizes which are unconventional anyway for parametric tests, or  perfect satisfaction of parametric assumptions). Some of the authors themselves seem to have studied such tests as alternatives to ANOVA, why not including such tests while at the same time advocating for others.
4: Type I and II error rates (or probabilities, if theoretical vs. empirical) indeed correspond to alpha and beta. Yet, a type I error is not a probability, although we estimate the probability of a type I error to occur given the model/method and observations. The first paragraph should therefore be rephrased. Although not an excuse, the same error also appear in the thus correctly referenced paper from 2010.
4: Defining the type II error as "accepting the null hypothesis" means the authors commit to the Neyman & Pearson approach to data testing (as in their 1933 paper), or a more Bayesian mindset (evidence accumulation and for instance likelihood ratios). Yet, experimental psychology usually commits to a mix of Neyman-Pearson and Fisher's approaches, or their mix with NHST (where accepting the null does not exists). A clarification of this choice would be useful for the naive experimental reader (since not coherently adopted throughout the entire paper). The latest review and tutorial by Perezgonzalez (2015) is in my opinion nice to be explicit on these aspects.
4: A reference to Rasch, Kubinger & Moder (2011) for testing violations of assumptions is provided, but not others from Moder. I was quite surprised, since Moder published several papers on violations in ANOVA, and for instance compared its statistics/performance to Hotellingís T2. 
4: Testing a parametric assumption using another test statistic is indeed known to be problematic, since we "aim" at accepting the null while measuring divergences or deviations (i.e. from normality or homoscedasticity, or even a t-distribution using a normality test...). While again aiming at accepting the null (thus Neyman-Pearson-ish), the review here does not integrate expectations based on (minimal) effect sizes (on deviations) that may affect the ANOVA test statistic (thus more Fisherian in this sense). This was one of Neyman-Pearson contributions, popularized more than 50 years later by Cohen in psychology. Turning to graphical methods (which is ok and practical in my opinion) simply means dropping the inferencial data-testing approach, and opting for descriptive statistics. A better reviewing of inferencial methods for testing deviations would be useful, or explicitly restricting the content of the paper to focus on statistical software for psychologists (as done in the referenced "Normality Tests for Statistical Analysis: A Guide for Non-Statisticians" for SPSS).
5: Again, Delacre et al. (2017) is referenced, but not much earlier papers leading to the same results (including those of Moder, see above). More surprinsingly, the reference itself does not point to any earlier research work on this issue (where there has been decades of both empirical and mathematical work, with the amount of MC simulations simply increasing in recent years).
5: Also, since the referenced paper (Delacre, 2017) compares t-test and Welch t-test with similar directions, and since the ANOVA is an omnibus test with contrasts coding allowing to project data and test trends/differences, how could we expect not to get the same overall tendancy here?
5: I can only guess the kurtosis definition given is that of DeCarlo (1997), but non-referenced. Also, see Westfall (2014) for a more statistically correct formulation (the title of the article is self-explaining: "Kurtosis as Peakedness, 1905 ñ 2014. R.I.P.")
6: RT distributions are right skewed, because the underlying measure is bounded (by 0) and the distribution cannot be normal (as a response distribution to a scale item cannot be normal either). It's thus rather because there is a left-wall and no right-wall, with possibly large RTs (sometimes then filtered out, or the distribution log-transformed to satisfy normality).
6: I could not make sense of the "One additional source of variability is the presence of unidentified moderators" sentence in the current context. While it is true (although there will always be unidentified moderators, else it would not be a statistical model analysis, but the complex reality), I could not relate it to the between-condition variability studied in this section. I looked into Delacre et al. (2017, p.95-4), which further improved my understanding of the page although being phrased almost identifcally, but which did not contain a reference to Cohen, Cohen, West, & Aiken (2003). The last paragraph on the page was more helpful, but it seems to simply imply randomization was incorrectly performed, or that very important factors to control (or moderators) where left out. If not highly correlated with each others, these factors should lead to the "noise" required to obtain normally distributed residuals, and thus for applying the type of tests considered here, or to slight deviations from homoscedasticity (thus again, the interpretation depends on the minimal effect size of interest for diagnostics). I must have severely misunderstood something here.
11: There is no step provided for SD-ratio, making it impossible to compute or understand the combinations / simulation plan.
11: What was in the end the criteria used to determine whether distributions are normal, same for homoscedastic? Hopefully (seeing the "round" numbers), these must be defined a priori (same SD, rnorm or something else) and not based on estimated statistics on simulated samples? From the paper, one cannot know if you implemented all tests/formulae yourselves (to avoid automatic corrections in some R wrappers), or did you use batching based on standard CRAN packages (in which case, which ones)? In this case, and since software evolves, which version was used (for which packages)?
11: Since you certainly can rely on statistics (or theoretical derivations from the probability distributions) to estimate deviation from normality of homoscedasticity, why not directly using such measures as continuous predictors? By splitting them into categories, distortions are introduced that affect all statistics (a statistical issue mathematically related to the actual focus of the paper).
11: Why running 1M simulations for each configuration (long and unecessary), while constraining so much the parameter range (and sets of values more generally). For instance, the effect of sample size tends to be exponential, and should be reflected by the chosen values.
11: Is normality met (or not) in all conditions, or just in a few (i.e. differences in distributions between-conditions, or between-simulations)? This can be expected from your assumptions/reasons for not considering normality/heteroscedasticity for granted (e.g. reaching a measure bound in one condition more than in another), and yet is not studied here?
11: What "other distributions", generalizations of the normal distribution (skewed normal, t...) or different ones (beta, gamma, exponential, Weilbull...)?
11: It is only by reading the code on https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fmdelacre%2FWelch-ANOVA&amp;data=02%7C01%7CD.Lakens%40tue.nl%7C582c8191b2104152871308d61fcf1bc1%7Ccc7df24760ce4a0f9d75704cf60efc64%7C1%7C0%7C636731371726480083&amp;sdata=zwANIXsovvbDijG31uPOghOFEPsHXUey0cgkqNRNmjA%3D&amp;reserved=0 that one gets all the information (e.g. on the strategy used to combine distributions and other parameters). Again, justification of how the simulation plan/design was chosen are sparse. Since not relying on natural t-test to ANOVA generalization, which allows to greatly limit the number of combinations required, minimal justification of the choices made would be welcome.
12: Although I commit to relying on median/MAD for position/dispersion estimation under weak constraints, non-normality does not seem like the right argument here. Mean/SD are not strongly associated to normal distributions (although we tend to interpret them on normal distributions in psychology for the sake of simplicity). Else, there would be no use to concentration inequalities (such as Chebyshev's inequality).
[...]
26: Figures 1 & 2 are missing from the paper (while tables are present). Since they were the only reason for dichotomizing the continuous statistics on normality/homoscedasticity, this is regretable. I thus hope it simply is a bug on my computer.
27: A whole literature is dedicated to the why one should always rely on the maximally powerful test statistic combined with visuals. This is especially true for departure from normality, with entire paper advocating for Shapiro-Wilk. These therefore are quite general and pre-existing recommendations. From a statistical perspective, and with a focus simply on power depending on distribution shape (thus deviations) see Yap & Sim (2011). Comparisons of various types of normality tests.
28: As mentioned previously, a priori power analysis is entailed by the approach presented at the beginning of the paper (Neyman-Pearson, controlling for type I error while aiming at maximal power).
28: How can balanced design be guaranteed? For many of the same reasons homoscedascity cannot be guaranteed (p.5-7), this is often not the case. Invoked factors in social experiments, attrition (with non missing at random -MAR- data, thus leading to larger deviations from asumptions) are only a few examples.
28: If balancing was indeed important, why not discussing this factor at all in the introduction. If not an assumption, ok, but then why not also considering distribution changes in shapes across conditions (which is the strongest factor of all, and usually impacts both asumptions which are at the core of the paper).

ref: Liu (2015) is present in the references, but not referenced in text, and yet the title seems quite relevant to the topic (Comparing Welch ANOVA, a Kruskal-Wallis test, and traditional ANOVA in case of heterogeneity of variance), and indeed is when reading through it.

Appendix: Given what is missing in the main text relatively to the simulations, the example of application of F / F* / W could be moved in SOMs. On the contrary, some elements in SOMs are not optional but mandatory to interpret any result.

SOM: Why adopting K-S test, when concluding S-W should be chosen in the main text?
SOM: The text provides some of the lacking information in the paper about the simulations (that should at least clearly point to the adequate SOM sections). Yet, part of this should be included in the main article to make interpretations minimally possible. Also, no discussion or reasons are provided for the different distributions tested.

Github: There is no git description or even readme file to explain how the code is structured. The names would be explicit, this would not be mandatory, but for instance "generate random datasets.R" (on which I focus below) contains code to apply the inferential tests, as well as most of the simulation plan.
Github: The fact that simulation data are generated and stored one by one by individual calls to the "get_simu" function (line 163 to 4011) is clearly error prone (and make the simulation non verifiable). If this R file is script generated (I hope so... while I could not find the source), I still don't see the interest but for obfuscation. Although the functions are useful for replication, the calls are not.
Github: The R code structure is also surprising unefficient (for simulations of such scale, or if users would want to extend the simulations, as it was in my case as a reviewer). Individual if/else tests are nested within the "get_sample" function (where a simple anonymous function or definition of parameters through local environments would have done the job of wrapping the calls to rnorm/rsnorm/...), itself being called within a double-for loop where vectors are incrementally built without pre-allocation...
Github: A few function definitions (for the distributions) and a simple expand/apply combination would have done the whole job in a few lines and much less time (even more with packages dedicated to mapping/aggregating on massive amounts of data). This would also allow far greater maintainability, especially extensibility.
Github: These issues are recurrent in all R scripts (with additional dependency to globally defined variables) and I add to look back in git commit history to get a sense of what was used in the paper rather than updated after article submission (although there are deletion/add operations that hide local changes).

--------------------------------------------------------
Vocabulary/grammar/understanding:
--------------------------------------------------------
3: "Type 1" -> Type I
5: "distribution. while"

--------------------------------------------------------
Notes (for the authors to check my understanding):
--------------------------------------------------------
a: limits of standard F for violations of normality or homoscedasticity => MC simulations with alternatives => implemented procedures, examples, recommendations
3: introduction to ANOVA and parametric assumptions
4: testing normality/equality of variance with a two-step procedure = bad/risky
5-7: normality/homoscedasticity as non-realistic
8-10: formulae
11-26: simulations and tables
27: recommendations = W-test by default + rely on descriptives + Shapiro-Wilk + a priori power analysis + balanced designs
------------------------------------------------------
________________________________________________________________________
International Review of Social Psychology
