---
title             : "Taking Parametric Assumptions Seriously: Arguments for the Use of Welch's *F*-test instead of the Classical *F*-test in One-way ANOVA"
shorttitle        : "W-test instead of F-test"

author: 
  - name          : "Marie Delacre" 
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "CP191, avenue F.D. Roosevelt 50, 1050 Bruxelles"
    email         : "marie.delacre@ulb.ac.be"
  - name          : "Christophe Leys"
    affiliation   : "1"
  - name          : "Youri L. Mora"
    affiliation   : "1"
  - name          : "Daniël Lakens"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Université Libre de Bruxelles, Service of Analysis of the Data (SAD), Bruxelles, Belgium"
  - id            : "2"
    institution   : "Eindhoven University of Technology, Human Technology Interaction Group, Eindhoven, the Netherlands "

authornote: |
  All materials required to reproduce the analyses reported in this article are available at https://osf.io/ru9tz/

  This work was supported by the Netherlands Organization for Scientific Research (NWO) VIDI grant 452-17-013.
  
  First author performed simulations. First, second and fourth authors contributed to the design. All authors contributed to the writing and the review of the literature. The Supplemental Material, including the full R code for the simulations and plots can be obtained from https://github.com/mdelacre/Welch-ANOVA. The authors declare that they have no conflicts of interest with respect to the authorship or the publication of this article.

abstract: |
  Parametric tests rely on two main assumptions: normality of the distribution and equality of variances. We argue that these assumptions are often unrealistic in the field of psychology. We underline the current lack of attention to parametric assumptions through an analysis of researchers' practices. Through Monte Carlo simulations we illustrate the consequences of performing the classic parametric *F*-test for ANOVA when the test assumptions are not met on the Type I error rate and statistical power. Under realistic deviations from the assumption of equal variances the classic *F*-test can yield severely biased results and lead to invalid statistical inferences. We examine two common alternatives to the *F*-test, namely the Welch's ANOVA (*W*-test) and the Brown-Forsythe test (*F\**-test). Our simulations show that the *W*-test is a better alternative and we therefore recommend using the *W*-test by default when comparing means. We provide a detailed example explaining how to perform the *W*-test in SPSS and R. We summarize our conclusions in five practical recommendations that researchers can use to improve their statistical practices.    
 
keywords          : "W-test; ANOVA; homogeneity of variance; statistical power; type I error, parametric assumptions"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figsintext: yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output:
  papaja::apa6_pdf:
    includes:
      after_body: "Appendix.tex"
---

  
```{r setup, include = FALSE}
library("papaja")
library("bookdown")
library("knitr")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r render_appendix, include=FALSE}
render_appendix("Appendix.Rmd")
```

When comparing independent groups researchers often analyze the means by performing a classical Analysis of Variance (ANOVA) *F*-test [@Erceg-Hurn_Mirosevich_2008]. The *F*-test relies on the assumptions that the data are sampled from a normal distribution, and that the data are sampled from distributions that have equal variances [or homoscedasticity; see @Lix_Keselman_Keselman_1996]. While a deviation from the normality assumption still yields quite robust conclusions when using *F*-test [@Glass_et_al_1972], unequal variances rapidly become problematic [@Grissom_2000]. Yet, researchers rarely provide information about these assumptions when they report an *F*-test. When we examined statistical tests reported in 116 articles in the *Journal of Personality and Social Psychology* published in the year 2016, fourteen percent of these articles reported a One-Way *F*-test, but only one article indicated to take the homogeneity of variances assumption into account by reporting corrected degrees of freedom for unequal variances, that could signal the use of the *W*-test instead of the classical *F*-test. A similar investigation [@Hoekstra_et_al_2012] yielded similar conclusions about the lack of attention to both the homoscedasticity and the normality assumptions. Despite the fact that the *F*-test is currently used by default, alternatives exist that are often a better choice, such as the Welch's *W* ANOVA (*W*-test), the Alexander-Govern test, James second order test and the Brown-Forsythe ANOVA (*F*\*-test). In this paper, we will discuss the pertinence of the assumptions of parametric tests, and provide simulations comparing *F*-test with the most adequate alternatives. As we argue in this article, the *W*-test has nearly the same statistical power than the *F*-test when variances are unequal, but provides better Type 1 error control. Since the *W*-test is available in practically all statistical software packages, researchers can improve their statistical inferences by replacing the *F*-test by the *W*-test.

# Normality and Homogeneity of variances in Ecological Conditions

For several reasons, assumptions of homogeneity of variances and normality are always more or less violated [@Glass_et_al_1972]. In this section we will summarize the specificity of the methods used in our discipline that can account for this situation. 

## Normality Assumption

It has been argued that there are many fields in psychology where the assumption of normality does not hold [@Cain_et_al_2016; @Micceri_1989; @Yuan_et_al_2004]. As argued by @Micceri_1989, there are many factors that could explain departures from the normality assumption. @Micceri_1989 identified many factors, and we will focus on three of them: the treatment effects, the presence of subpopulations within the studied one and the bounded measures underlying residuals.

First, while it is obvious that the mean can be changed by the treatment effects, experimental treatment could also change the shape of a distribution, either on one or both indicators of skewness, a measure of asymmetry of the shape of the distribution, and kurtosis, a measure of the tendency to produce extreme values \footnote{a distribution with positive kurtosis will have heavier tails than the normal distribution meaning that extreme values will be more likely, while a distribution with negative kurtosis will have lighter tails than the normal distribution meaning that extreme values will be less likely.}[@Wilcox_2005;@Westfall_2014]. For example, @Knapp_and_Dixon_1950 show that practicing juggling during an amount of time can have various effect of juggle learning efficiency between subjects. This could influence the kurtosis of the distribution. 
 
Second, prior any experimental treatments, departures from the normality assumptions may be due to the presence of several subpopulations that present unequal characteristics and that are not controlled within the studied group (resulting in mixed distributions). This unavoidable lack of control is inherent of our field given its complexity. As an illustration, @Wilcox_2005 writes that pooling two normally-distributed populations that have the same mean but different variances (e.g. normally distributed scores for schizophrenic and not schizophrenic participants) could result is distributions that are very similar to the normal curve but with thicker tails. As another example, when assessing a wellness score for the general population, data may be sampled from a left-skewed distribution, because most people are probably not depressed [see @Heun_et_al_1999]. In this case, people suffering from depression and people that do not are included within the same population group, explaining the asymmetry.   

Third, bounded measure can also explain the non-normal distributions. Such examples can be found in the field of neurosciences such as reaction times, that can be very large but never below zero (resulting in right-skewed distributions; see @Ratcliff_1979 for a discussion on the reaction time distribution shape). In sum, there are many common situations in which normally distributed data is an unlikely assumption.

## Homogeneity of Variances Assumption

Homogeneity of variances (or homoscedasticity) is a mathematical demand that is ecologically very unlikely [@Erceg-Hurn_Mirosevich_2008; @Grissom_2000]). 

In a previous paper [@Delacre_et_al_2017], we identified three different causes of heteroscedasticity: the variability inherent to the use of measured variables, the variability induced by quasi-experimental treatments on measured variables, and the variability induced by different experimental treatments on randomly assigned subjects. One additional source of variability is the presence of unidentified moderators [@Cohen_et_al_2013].

First, psychologists, as many scholars from various fields in human sciences, often use measured variables (e.g. age, gender, educational level, ethnic origin, depression level, etc.) instead of random assignment to conditions. Prior to any treatment, parameters of pre-existing groups can vary largely from one population to another, as suggested by @Henrich_et_al_2010. For example, @Green_et_al_2005 have shown that the scores of competitiveness, self-reliance and interdependence are more variable in some ethnic groups than in others. This stands true for many pre-existing groups such as gender, cultures, or religions and for various outcomes [see for example @Adams_et_al_2014; @Beilmann_et_al_2014; @Church_et_al_2012; @Cohen_and_Hill_2007; @Haar_et_al_2014; @Montoya_Briggs_2013]. Moreover, groups are sometimes defined with the intention to have different variabilities. For example, as soon as a selective school admits its students based on the results of aptitude tests, the variability will be smaller compared to a school that accepts all students. In this example, the goal is not to alter the variability but is an inherent statistical implication of such theoretical positions.

Second, a quasi-experimental treatment can have different impacts on variances between pre-existing groups, that can even be of theoretical interest. For example, in the field of linguistics and social psychology, @Wasserman_and_Weseley_2009 investigated the impact of language gender structure on sexist attitudes of women and men. They tested differences between sexist attitude scores of subjects who read a text in English (i.e. a language without grammatical gender) or in Spanish (i.e. a language with grammatical gender). The results showed that (for a reason not explained by the authors), the women's score on the sexism dimension was more variable when the text was read in Spanish than in English ($SD_{spanish}=.80 > SD_{english}=.50$).For men, the reverse was true ($SD_{spanish}=.97 < SD_{english}=1.33$) \footnote{Note that this example is for didactic reasons, the differences have not been tested and might not differ significantly}  

Third, even when the variances of groups are the same before treatment (due to a complete randomization in the group assignment), unequal variances can emerge later, as a consequence of an experimental treatment [@Bryk_and_Raudenbush_1988;@Cumming_2013; @Erceg-Hurn_Mirosevich_2008;@Keppel_and_Wickens_2004;@Box_1954]. For example, @Koeser_and_Sczesny_2014 have compared arguments advocating either masculine generic or gender-fair language with control messages in order to test the impact of these conditions on the use of gender-fair wording (measured as a frequency). They report that the standard deviations increase after treatment in all experimental conditions.

Fourth, more often than not, psychological processes are captured in situations where many variables are unidentified and/or left uncontrolled [@Cohen_et_al_2013]. Since some of these variables can act as moderators, they can generate heteroscedasticity.Indeed,  by definition, a moderator is a variable that will interact with factors, which implies that the effect of the moderator will be different in one condition of the factor than in another condition of the same factor.   

To conclude, there are many common situations in which the homogeneity of variances assumption is unlikely to be true. 

# Stakes Underlying the *F*-test Assumptions of Normality and Homogeneity of variances

Assumptions violations would not be a matter per se, if the *F*-test was perfectly robust against departures from them [@Glass_et_al_1972]. When performing a test, two types of error can be made: Type I error and Type II error. Type I error consists in falsely reject the main hypothesis in favour of an alternative hypothesis, and Type I error rate ($\alpha$) is the proportion of tests, when sampling many times from the same population, that wrongly reject the main hypothesis in favour of an alternative hypothesis [@Perezgonzalez_2015]. Type II error consists in wrongly reject the alternative hypothesis and type II error rate ($\beta$) is the proportion of tests, when sampling many times from the same population, that wrongly reject the alternative hypothesis [@Perezgonzalez_2015]. Finally, the statistical power (1-$\beta$) is the proportion of tests, when sampling many times from the same population, that correctly reject the main hypothesis in favour of an alternative hypothesis.

## Violation of the Normality Assumption

Regarding the type I error rate, the distribution shape has a very little impact on the *F*-test [@Harwell_et_al_1992]. When departures are very small (i.e. a kurtosis between 1.2 and 3 or a skewness between -.4 and .4), the Type I error rate of the *F*-test is very close of expectations even with very small samples of 11 subjects per group [@Hsu_and_Feldt_1969].     

Regarding the type II error rate, many author underlined that departures from normality does not seriously affect the power [@Tiku_1971; @David_and_Johnson_1951; @Harwell_et_al_1992; @Srivastava_1959; @Boneau_1960; @Glass_et_al_1972]. However, following @Srivastava_1959 and @Boneau_1960, kurtosis impacts slightly more the power than skewness; the effect of non-normality on power increases when n are unequal between groups [@Glass_et_al_1972]; lastly the effect of non-normality decreases when n increases [@Srivastava_1959].    

## Violation of Homogeneity of Variances Assumption

Regarding the type I error, the *F*-test is sensitive to unequal variances [@Harwell_et_al_1992]. More specifically, the higher the *SD*-ratio, the higher the impact. When there are only two groups, the impact is smaller than when there are more than two groups [@Harwell_et_al_1992]. When there are more than two groups, the *F*-test becomes more liberal, even when sample sizes are equal across groups [@Tomarken_and_Serlin_1986]. Moreover, when sample sizes are unequal, there is a strong effect of the sample size and variance pairing: in case of positive pairing (i.e. the group with the larger sample size also has the larger variance), the test is too conservative whereas in case of negative pairing (i.e. the group with the larger sample size has the smaller variance), the test is too liberal [@Glass_et_al_1972; @Nimon_2012; @Overall_et_al_1995; @Tomarken_and_Serlin_1986].   

Regarding the type II error, there is a small impact of unequal variances when sample sizes are equal [@Harwell_et_al_1992], however, there is a strong effect of the sample size and variance pairing [@Nimon_2012; @Overall_et_al_1995]. In case of positive pairing, the type II error rate increases (i.e. the power decreases) and in case of negative pairing, the type II error decreases (i.e. the power increases). 

## Cumulative violation of normality and Homogeneity of Variance

Regarding both type I ad type II error, following @Harwell_et_al_1992, there is no interaction between normality violations and unequal variances. Indeed, the effect of heteroscedasticity is relatively constant regardless of the shape of the distribution. 

Throughout mathematical explanations and Monteo Carlo simulations, among the five previously mentioned alternatives, we chose to examine the *F*-test, *W*-test and *F*\*-test and to exclude the James second-order and Alexander-Govern's test. @Tomarken_and_Serlin_1986 have shown that from the available alternatives, the *F*\*-test and the *W*-test are the best choices. Both tests are available in SPSS, which is a widely used software in psychological sciences [@Hoekstra_et_al_2012]. The two later tests were not included, because they yield very similar results to the *W*-test t but are less readily available in statistical software packages. For a more extended description of these two alternatives, see @Schneider_and_Penfield_1997. 

# The Mathematical Differences Between the *F*-test, *W*-test, and *F*\*-test

In this section, we will explain the mathematical differences in how the *F*-test, *W*-test and *F*\*-test are computed, with a focus on the way standard deviations are pooled across groups to stress the implications on heteroscedasticity (see appendix for a numerical example). We suggest readers to focus their attention on the terms involving the variances.

As shown in \@ref(eq:Fstat) The *F* statistic is calculated by dividing the inter-group variance by a pooled error term, where $s_{j}^2$ and $n_{j}$ are respectively the variance estimates and the sample sizes from each independent group, and where *k* is the number of independent groups: 

\begin{equation} 
F=\frac{\frac{1}{k-1}\sum_{j=1}^k [n_j(\bar{x_{j}}-\bar{x_{..}})^2]}{\frac{1}{N-k}\sum_{j=1}^k(n_j-1)s_j^2}
(\#eq:Fstat)
\end{equation} 

The degrees of freedom in the numerator \@ref(eq:FnumDF) and in the denominator \@ref(eq:FdenomDF) of the *F*-test are computed as follows: 

\begin{equation} 
df_n=k-1
(\#eq:FnumDF)
\end{equation} 

\begin{equation} 
df_d= N-k, 
(\#eq:FdenomDF)
\end{equation} 

With $N=\sum_{j=1}^k n_j$. As a generalization of the Student's *t*-test, the *F*-test is calculated based on a pooled error term. This implies that all samples are considered as issued from a common population variance (hence the assumption of homoscedasticity). 

When there is heteroscedasticity, if the larger variance is associated with the larger sample size, the error term, denominator in \@ref(eq:Fstat), is overestimated. *F*-value is therefore decreased, leading to fewer significant findings than expected with a specific Type I error level. It explains why the *F*-test is too conservative in this configuration. 

On the other side, when the larger variance is associated with the smaller sample size, the error term, denominator in \@ref(eq:Fstat), is underestimated. *F*-value is therefore inflated, yielding more significant results than expected under the nominal Type I error level.

The *F*\* statistic proposed by @Brown_and_Forsythe_1974 is computed as follows:

\begin{equation} 
F^*= \frac{\sum_{j=1}^k[n_j(\bar{x_j}-\bar{x_{..}})^2]}{\sum_{j=1}^k [(1-\frac{n_j}{N})s_j^2]}
(\#eq:BFstat)
\end{equation} 

Where $x_j$ and $s_j^2$ are respectively the group mean and the group variance, and $\bar{x_{..}}$ is the overall mean. 

As it can be seen in \@ref(eq:BFstat) the numerator of the *F*\* statistic is equal to the sum of squares between groups (which is equal to the numerator of the *F* statistic when one compares two groups). In the denominator, the variance of each group is weighted by 1 minus the relative frequency of each group. This adjustement implies that the variance associated with the group with the smallest sample size is given more weight than in the *F*-test. As a result, when the larger variance is associated with the larger sample size, *F*\* is larger than *F*, because the denominator decreases, leading to more significant findings compared with the *F*-test. On the other hand, when the larger variance is associated with the smaller sample size, *F*\* is smaller than *F*, because the denominator increases, leading to fewer significant findings than expected with the *F*-test. The degrees of freedom in the numerator and in the denominator of *F*\*-test are computed as follows (with the same principle than the denominator computation):

\begin{equation} 
df_n= k-1
(\#eq:BFnumDF)
\end{equation} 

\begin{equation} 
df_d= \frac{1}{\sum_{j=1}^k[\frac{(\frac{(1-\frac{n_j}{N})s_j^2}{\sum_{j=1}^k[(1-\frac{n_j}{N})s_j^2]})^2}{n_j-1}]}
(\#eq:BFdenomDF)
\end{equation} 

\@ref(eq:Wstat) provides the *W*-test computation. In the numerator of the *W*-test, the squared deviation between group means and the general mean are weighted by $\frac{n_j}{s_j^2}$ instead of $n_j$ [@Brown_and_Forsythe_1974].As a consequence, for equal sample sizes, the group with the highest variance will have smaller weight [@Liu_2015]. The demonstration of Welch's theorem is out of the scope of this paper, however, it is interesting to understand the consequences. When the biggest sample size has the biggest variance, as previously mentioned, the *F*-test will be too conservative, the Welch corrects that even better than the *F*\* such as *W* > *F*\* > *F*. Conversely, when the smallest sample size has the biggest variance, the *F*-test will be too liberal, the Welch also corrects the effect even better than the *F*\* such as *W* < *F*\* < *F*.
 
\begin{equation} 
W=\frac{\frac{1}{k-1}\sum_{j=1}^k[w_j(\bar{X_j}-\bar{X'})^2]}
{1+\frac{2(k-2)}{k^2-1}\sum_{j=1}^k[(\frac{1}{n_j-1})(1-\frac{w_j}{w})^2]}
(\#eq:Wstat)
\end{equation} 

where:
$$w_j=\frac{n_j}{s_j^2}$$
$$w=\sum_{j=1}^k(\frac{n_j}{s_j^2})$$
$$\bar{X'}=\frac{\sum_{j=1}^k(w_j\bar{x_j})}{w}$$

The degrees of freedom of the *W*-test are approximated as follows:

\begin{equation} 
df_n= k-1
(\#eq:WnumDF)
\end{equation} 

\begin{equation}
df_d= \frac{k^2-1}{3\sum_{j=1}^k[\frac{(1-\frac{w_j}{w})^2}{n_j-1}]}
(\#eq:WdenomDF)
\end{equation} 

When there are only two groups to compare, the *F*\*-test and *W*-test are identical (i.e., they have exactly the same statistical value, degrees of freedom and significance). However, when there are more than two groups to compare, the tests differ. To better understand how to compute all statistics, a set of fictional raw data simulating the example of a three-group design is available in the appendix. The following section will present Monte Carlo simulations assessing these three tests on both Types I and II error rates.

# Monte Carlo simulations: *F*-test vs. *W*-test vs. *F*\*-test 

We chose to examine the Type I error rate and the statistical power of the *F*-test, *W*-test and *F*\*-test. The James second-order test and the Alexander-Govern's test were not included, because they yield very similar results to the *W*-test but are less readily available in statistical software packages. For a more extended description of these two alternatives, see @Schneider_and_Penfield_1997.

We conducted simulations for 2560 scenarios that we deemed most relevant. For different distributions underlying the data, $k-1$ samples (where *k* varied from 2 to 5) were generated from a population where $\sigma_j=2$ and sample sizes ($n_j$) were 20, 30, 40, 50 or 100. The *SD* and the sample size of the last group was a function of the sample sizes ratio (*n*-ratio =$\frac{n_j}{n_1}$; ranging from 0.5 to 2 in steps of 0.5) and the *SD*-ratio (ranging from .5 to 4). This setup resulted in a wide range of conditions in which the normality assumption was met or not, and where the homoscedasticity assumption was met or not, as summarized in Table 1.

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/Administrateur/Documents/GITHUB/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/Table 1.PNG")
```

## Type I Error Rate of the *F*-test vs. *W*-test vs. *F*\*-test

### Simulating error rates when the normality assumption is met. 

We simulated 1,000,000 data sets under the null hypothesis (all means are equal across groups) and computed the p-value distribution across the three tests under 320 scenarios where the normality assumption is met (see Table 1). When the null-hypothesis is true, the *p*-value distribution should be uniform, and the Type I error rate is supposed to be equal to the alpha level (here 5%). As explained above, when comparing two groups, the *W*-test and *F*\*-test are mathematically identical and should yield identical error rates. 

We classified the 320 scenarios where the normality assumption is met into five categories. Previous findings highlighted differences in terms of the Type I error rate of the *F*-test as a function of the correlation between sample sizes and *SD*s [see @Nimon_2012; @Overall_et_al_1995]. Therefore, scenarios where variances are unequal between groups were divided into three categories: 1) sample sizes are unequal and there is a positive correlation between sample sizes and *SD*s (i.e. the group with the biggest sample size has the biggest *SD*), 2) sample sizes are unequal and there is a negative correlation between sample sizes and *SD*s or 3) sample sizes are equal across all groups. In the last two categories of simulations, 4) variances are equal between groups and sample sizes are unequal or 5) variances are equal between groups and sample sizes are equal. For each category, the sample size of the first group was varied from 20 to 100 and the number of groups in the ANOVA varied from 2 to 5. Table 2 reports the main conclusions based on a sample size of 30 in the first group (which seemed ecological and didactic) and provides Type 1 error rates for ANOVAs with two and three groups (for results from additional scenarios, please see the Supplemental Material). Because the Type I error rates were not normally distributed in each category, we report the median and the Median Absolute Deviation (*MAD*), a more robust measure of central tendency and dispersion than the mean and *SD* [@Leys_et_al_2013]. There is no measure of dispersion for the fifth category because there was only one condition where both equal variances, equal sample sizes, and normality were met, when there are 30 subjects in the first group. 

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/Administrateur/Documents/GITHUB/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/Table 2.PNG")
```

Table 2 shows that, concerning the Type I error, in all cases, the *W*-test performs as well as or better than the two alternatives. More specifically, as soon as variances are unequal between groups (categories 1 to 3), the *W*-test stays unaltered whereas the two other tests become unreliable. 

These results can be generalized: when the number of groups increases, the *F*-test becomes increasingly unreliable. The Type I error rate is too low when there is a positive correlation between sample sizes and population standard deviations, but too high when there is either a negative correlation between sample sizes and population standard deviations or unequal variances with equal sample sizes \footnote{To yield a robust test, the Type I error rate has to be sufficiently close to the nominal 5% level. In order to assess the robustness of the three tests in our simulations, we follow Bradley (1978) and consider the Type I error rate as 'close enough' to the nominal 5% if it falls in the interval [0.025; 0.075]}. The *F*\*-test is robust against unequal population variances when there are two groups to compare (Table 2). When there are three groups to compare, the test is less affected by violations of the assumption of equal variances than the *F*-test, but the Type I error rate still increases when there are unequal population variances between groups. Additional simulations, presented in the Supplemental Material, show that the test gets more liberal as the sample sizes are smaller, and as the *SD*-ratio and the number of groups to compare increase. Finally, the *W*-test yields a more stable Type I error rate, regardless of the number of groups that are compared, and regardless of the *SD*-ratio. 

### Simulating Type I error rates when the normality assumption is not met. 
We tested the impact of non-normal distributions without heterogeneity of variances on the Type I error rate, based on 560 scenarios (See Table 1). As an illustration, Table 3 contains information about the median Type I error rate and associated *MAD* of categories where we compare two or three groups of different sample sizes, extracted from 1) mixed normal distributions (symmetric with heavy tails distributions), 2) normal right-skewed distributions or 3) chi-squared distributions, when there are 30 subjects in the first group (for more scenarios, see the Supplemental Material). Table 3 shows that across the three categories, the *F*-test provides a better control of the Type I error rate than both *W*-test and *F*\*-test. When there are two groups to compare, the *F*\*-test and *W*-test have the same Type I error rate since, as stated above, both tests are identical. However, when there are three groups to compare, the *W*-test is more conservative than the *F*-test and *F*\*-test with heavy tailed distributions, and more liberal than the *F*-test and *F*\*-test with skewed distributions. Lastly, we underline that, in these scenarios, departures from normality have mild consequences.

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/Administrateur/Documents/GITHUB/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/Table 3.PNG")
```

In general, while the *W*-test is more robust than both the *F*-test and *F*\*-test when distributions are normal and groups have unequal variances, it is less robust than the two other tests when the normality assumption is not met but groups have equal variances (Supplemental Material 2). The *W*-test is more affected by heavy-tailed and skewed distributions than the *F*-test, becoming slightly more conservative with heavy-tailed distributions, and more liberal with skewed distributions. Furthermore, the *W*-test becomes more liberal when highly skewed distributions are combined with unequal variances, particularly when sample sizes are unequal between groups, but it is still less liberal than the *F*-test (see Table 4).

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/Administrateur/Documents/GITHUB/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/Table 4.PNG")
```

In sum, taking all our simulations \footnote{All data are available on the following link: https://osf.io/ru9tz/ (See Type I error rate.xlsx)} into account, we can draw the following conclusions for the Type I error rate: 

1. With departure from normality but homoscedasticity, the *F*-test remains acceptable with at least 20 subjects per group, following @Bradley_1978's acceptable interval of an error rate between 025 to .075 when the alpha is set to 0.05.
2. As soon as there is heteroscedasticity, regardless of normality, the Type I error rate of the *F*-test and *F*\*-test will commonly fall outside of Bradley's interval, even with big sample sizes. The Type I error rate of the *W*-test will generally fall inside of the Bradley's interval as long as there are at least 20 subjects per groups, except when distributions are highly skewed, as discussed in the next point. 
3. For highly skewed distributions, which are easily detectable based on a Shapiro-Wilk test, the *W*-test is the best choice, but one needs to collect at least 50 subjects per group to guarantee a Type 1 error rate that falls within Bradley's interval. With less than 50 subjects per group, other options such as non-parametric approaches [see, e.g., @Leys_Schumann_2010] are advised. 

## Statistical power for the *F*-test, *W*-test, and *F*\*-test

In addition to the Type 1 error rate, the statistical power of a test is important to consider. The power of a test is a function of the effect size, the sample size, and the nominal alpha level [@Cohen_1988]. We performed simulations in which we introduced a true effect (the mean = 1 in the last group, mean = 0 in all other groups). We again manipulated the distributions, variances and *SD*-ratios of the populations from which groups are extracted as well as sample sizes and sample ratios, as was done when examining the Type 1 error rate. Based on these results, we computed the relative power difference of the *W*-test and *F*\*-test in comparison with the *F*-test within categories, as follows: 

\begin{equation} 
Relative~difference~of~power~of~the~W-test=\frac{Power_{W-test}-Power_{F-test}}{Power_{F-test}}
(\#eq:RDPWtest)
\end{equation} 

\begin{equation} 
Relative~difference~of~power~of~the~F^*-test=\frac{Power_{F^*-test}-Power_{F-test}}{Power_{F-test}}
(\#eq:RDPBFtest)
\end{equation} 

Formulas \@ref(eq:RDPWtest) and \@ref(eq:RDPBFtest) will return a positive value when the *W*-test (or *F*\*-test) is more powerful than the *F*-test, and they will return a negative value when the *W*-test (or *F*\*-test) is less powerful than the *F*-test.

First, it is often argued that the *W*-test and *F*\*-test are less powerful than the *F*-test when the assumptions of normality and equal variances are met. However, our simulations show that under these assumptions, the loss of power is always smaller than 3% of the power of the *F*-test. Moreover, the relative differences in power between the *F*-test and both *W*-test and *F*\*-test tend towards zero when the number of subjects per group increases (Table 5).

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/Administrateur/Documents/GITHUB/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/Table 5.PNG")
```

Second, when the assumption of normality is met but variances are unequal, since the *F*-test becomes either too liberal or too conservative, depending on the correlation between *ns* and *SD*s, the *W*-test is sometimes more powerful, sometimes less powerful than the *F*-test (Table 6). However, the *W*-test is preferable since (a) it is more stable; and (b) its power is closer than the expected power (computed using the Welch's power curve, Minitab assistance, *n.d*). 

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/Administrateur/Documents/GITHUB/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/Table 6.PNG")
```

Third, when the assumption of normality is not met but there is homoscedasticity, the *W*-test tends to be a little more powerful than the *F*-test, especially when distributions are highly skewed (Table 7). This is due to the fact that the *W*-test is more liberal. Therefore, whenever researchers have good reasons to think that variances are equal between groups and given that the departure from normality does not impact the *F*-test too much, the classical *F*-test would be a better choice in these circumstances. However, since we argue that (a) heteroscedasticity is often present but can be difficult to detect and (b) the *W*-test and *F*-test have very similar power when homoscedasticity is met, we still advice to use the *W*-test. 

```{r "", echo=FALSE, out.width = '100%',fig.cap = ""}
knitr::include_graphics("C:/Users/Administrateur/Documents/GITHUB/W-ANOVA/Rmarkdown folder/Rmarkdown inputs/Table 7.PNG")
```

Note that globally, when the assumption of homoscedasticity is met but distributions are not normal, and more specifically with heavy tailed distributions, we observed results that are in contrast with previous work [@Wilcox_1998] for all tests comparing means. @Wilcox_1998 concluded that there is a loss of power when comparing means from heavy-tailed distributions (e.g. double exponential or a mixed normal distribution) when compared to normal distributions. This finding is based on the argument that heavy-tailed distributions are associated with bigger standard deviations than normal distributions, and that the effect size for such distributions is therefore smaller [@Wilcox_2011]. However, it is important to avoid the confusion between kurtosis and the standard deviation. [@DeCarlo_1997] explains that kurtosis and *SD* are completely independent, meaning that one can find distributions that have similar *SD* but different kurtosis. We ran simulations manipulating kurtosis while keeping the *SD* unaltered (by comparing a normal distribution with a mixed and a double-exponential distribution that have different kurtosis, but the same *SD*). Results show that when heavy-tailed distributions have equal standard deviations and *SD*-ratios as normal distributions, there are no substantial differences in power as a function of the kurtosis of the underlying distribution (see Supplemental Material 3).

Finally, when both the normality and equal variances assumptions are not met, the *F*-test is never reliable. However, the *W*-test is not very reliable either (see Supplemental Material 3). Indeed, the *W*-test will be more powerful than the *F*-test, but again, just because it is too liberal (see the conclusions in the Type I error rate section). Therefore, under these precise circumstances, we recommend researchers switch to non-parametric alternatives [@Leys_Schumann_2010]. 

#ADD FIGURE 1 AND 2 HERE

# Recommendations

In sum, we provide five recommendations:

1. Use the *W*-test instead of the *F*-test to compare groups means. The *F*-test and *F*\*-test should be avoided, because the equal variances assumption is often unrealistic, tests of the equal variances assumption will often fail to detect differences when these are present, the loss of power when using the *W*-test is very small (and often even negligible), and the gain in Type I error control is considerable under a wide range of realistic conditions.
2. Do not neglect the descriptive analysis of the data. A complete description of the shape and characteristics of the data (e.g. histograms and boxplots) is important. When at least one statistical parameter relating to the shape of the distribution (e.g. variance, skewness, kurtosis) seems to vary between groups, comparing results of the *W*-test with results of a nonparametric procedure is useful in order to better understand the data.  
3. Use the Shapiro-Wilk test to detect departures from normality (combined with graphical methods). Contrary to the Kolmogorov-Smirnov test, the Shapiro-Wilk test will almost always detect distributions with high skewness, even with very small sample sizes. With small sample sizes, the *W*-test will not control Type I error rate when skewness is present and detecting departures for normality is therefore especially important in small samples. When comparing at most four groups, the *W*-test should be avoided if the Shapiro-Wilk test reject the normality assumption, with less than 50 observations per group. When comparing more than four groups, the *W*-test should be avoided if the Shapiro-Wilk test rejects the normality assumption, with less than 100 subjects per group When normality cannot be assumed because of high kurtosis or high skewness, we recommend the use of alternative tests that are not based on means comparison, such as the trimmed means test [@Erceg-Hurn_Mirosevich_2008] \footnote{The null hypothesis of the trimmed means test assumes that trimmed means are the same between groups. A trimmed mean is a mean computed on data after removing the lowest and highest values of the distribution . Trimmed means and means are equal when data are symmetric. On the other hand, when data are asymmetric, trimmed means and means differ.} or nonparametric tests. For more information, see @Erceg-Hurn_Mirosevich_2008.

4. Perform a-priori power-analyses. Fifty subjects per group are generally enough to control the Type I error rate, but power analyses are still important in order to determine the required sample sizes to achieve sufficient power to detect a statistically significant difference [see @Albers_and_Lakens_2018]. 

5. Use balanced designs (i.e. same sample size in each group) whenever possible. When using the *W*-test, the Type I error rate is a function of criteria such as the skewness of the distributions, and whether skewness is combined with unequal variances and unequal sample sizes between groups. Our simulations show that the Type I error rate control is in general slightly better with balanced designs. 
 
  
